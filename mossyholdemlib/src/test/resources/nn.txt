FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=5 9 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (5, 4, 5.00000000000000000000e-01) (5, 4, 5.00000000000000000000e-01) (5, 4, 5.00000000000000000000e-01) (5, 4, 5.00000000000000000000e-01) (5, 4, 5.00000000000000000000e-01) (5, 4, 5.00000000000000000000e-01) (5, 4, 5.00000000000000000000e-01) (5, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (9, 4, 5.00000000000000000000e-01) (9, 4, 5.00000000000000000000e-01) (9, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -5.74564673006534576416e-02) (1, -7.08534121513366699219e-02) (2, -3.33541482686996459961e-02) (3, 1.68133527040481567383e-02) (4, -1.80005356669425964355e-02) (0, 6.49679377675056457520e-02) (1, -8.37105587124824523926e-02) (2, 7.66554996371269226074e-02) (3, -5.10193482041358947754e-02) (4, -8.21614041924476623535e-02) (0, -8.67248177528381347656e-02) (1, 1.59724801778793334961e-02) (2, 4.94580343365669250488e-02) (3, 4.12656590342521667480e-02) (4, -4.80753295123577117920e-02) (0, -2.10653245449066162109e-03) (1, -4.54185158014297485352e-03) (2, 6.51184543967247009277e-02) (3, 4.59175631403923034668e-02) (4, -6.36137723922729492188e-02) (0, 4.33730855584144592285e-02) (1, -2.86312922835350036621e-02) (2, -6.12258911132812500000e-03) (3, 9.76345762610435485840e-02) (4, -5.55515773594379425049e-02) (0, -5.53587675094604492188e-02) (1, -1.47957950830459594727e-02) (2, -7.28505998849868774414e-02) (3, 4.15071845054626464844e-05) (4, 9.77153405547142028809e-02) (0, -9.82634872198104858398e-02) (1, 8.55499580502510070801e-02) (2, 3.82058843970298767090e-02) (3, -7.37217739224433898926e-02) (4, -4.18102815747261047363e-02) (0, 9.46042910218238830566e-02) (1, 1.43035054206848144531e-02) (2, -9.27150249481201171875e-04) (3, 1.73880904912948608398e-02) (4, 4.17567566037178039551e-02) (5, 5.97839057445526123047e-03) (6, 7.89301767945289611816e-02) (7, -2.05469578504562377930e-02) (8, 6.73410817980766296387e-02) (9, 1.44569575786590576172e-03) (10, 9.77859124541282653809e-02) (11, 8.79120007157325744629e-02) (12, -6.31261616945266723633e-02) (13, 3.86852100491523742676e-02) (5, -1.76172703504562377930e-02) (6, -9.34339612722396850586e-02) (7, 5.54270371794700622559e-02) (8, -3.78005616366863250732e-02) (9, 8.59720185399055480957e-02) (10, -6.83283358812332153320e-02) (11, 5.63560426235198974609e-03) (12, -8.22543352842330932617e-02) (13, -4.85362820327281951904e-02) (5, 5.06613329052925109863e-02) (6, 6.48135617375373840332e-02) (7, -7.83006697893142700195e-02) (8, 6.68063759803771972656e-04) (9, 2.81718745827674865723e-02) (10, 8.47534611821174621582e-02) (11, 5.14955446124076843262e-02) (12, 8.54801759123802185059e-02) (13, 6.53352215886116027832e-02) 
